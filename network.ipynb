{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ada65a6d9471272",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 导入所需库\n",
    "\n",
    "主要包括os、PyTorch、scikit-learn、numpy、pandas和tqdm。并检查是否有GPU可用并设置计算设备。\n",
    "注意默认使用GPU0，需要更换计算设备调节 `cuda:0` 变量即可，cuda使用英伟达显卡计算，Mac系统并不支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:13.990569400Z",
     "start_time": "2024-06-02T05:02:13.986128700Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 检查是否有GPU可用并指定设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149528293620c12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521e67c2e8f1dc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据读取与处理函数\n",
    "\n",
    "* 读取数据：从CSV文件中读取数据，并将第一列转换为时间索引。\n",
    "* 过滤数据：根据提供的起止日期，过滤指定时间段的数据。\n",
    "* 提取特征：提取目标变量Y、特征变量X和F，以及其他宏观变量M。\n",
    "* 返回数据：返回处理后的数据，包括Y、X、F、M和时间索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "861ac67ceafb5f93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:15.150231300Z",
     "start_time": "2024-06-02T05:02:15.136251400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data(filename, start_date=None, end_date=None):\n",
    "    # 读取数据\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # 将第一列设置为时间索引并转换为日期类型\n",
    "    df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'], format='%Y%m')\n",
    "    df.set_index('Unnamed: 0', inplace=True)\n",
    "\n",
    "    # 如果提供了起止日期，则截取该时间段的数据\n",
    "    if start_date and end_date:\n",
    "        df = df.loc[pd.to_datetime(start_date, format='%Y%m'):pd.to_datetime(end_date, format='%Y%m')]\n",
    "\n",
    "    # 读取Y数组（以xr_开头的变量）\n",
    "    Y = df.filter(regex='^xr_').values\n",
    "\n",
    "    # 读取F数组（列名以m结尾的数据）\n",
    "    F = df.filter(regex='m$').values\n",
    "\n",
    "    # 读取X数组（以f_开头的变量）\n",
    "    X = df.filter(regex='^f_').values\n",
    "\n",
    "    # 读取M数组（其他变量）\n",
    "    M = df.drop(columns=df.filter(regex='(^xr_)|(^f_)|m$').columns).values\n",
    "\n",
    "    # 返回时间索引列\n",
    "    time = df.index\n",
    "    return Y, X, F, M, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444fb52a8edaa6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 计算R2OOS的函数\n",
    "* 计算条件均值预测：计算条件均值预测并滞后一期。\n",
    "* 计算残差平方和和总平方和：计算残差平方和（SSres）和总平方和（SStot）。\n",
    "* 计算R2OOS分数：返回R2OOS分数。\n",
    "逻辑与原文中保持了一致，对于实际计算中这部分是否合理的讨论，见discussion.pdf。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663d43c35967758c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:15.857656600Z",
     "start_time": "2024-06-02T05:02:15.849531200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def R2OOS(y_true, y_forecast):\n",
    "    # 计算条件均值预测\n",
    "    y_condmean = np.divide(y_true.cumsum(), (np.arange(y_true.size) + 1))\n",
    "\n",
    "    # 滞后一期\n",
    "    y_condmean = np.insert(y_condmean, 0, np.nan)\n",
    "    y_condmean = y_condmean[:-1]\n",
    "    y_condmean[np.isnan(y_forecast)] = np.nan\n",
    "\n",
    "    # 计算残差平方和和总平方和\n",
    "    SSres = np.nansum(np.square(y_true - y_forecast))\n",
    "    SStot = np.nansum(np.square(y_true - y_condmean))\n",
    "\n",
    "    return 1 - SSres / SStot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbb64e680b115d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 定义文件读写操作的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00db42ae3bbaf95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:16.615227600Z",
     "start_time": "2024-06-02T05:02:16.607125200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def save_model_config(model, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8c8d482847ef6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义全局变量\n",
    "* 用于控制网络结构，定义模型结构参数，包括隐藏层数、隐藏节点数和输出节点数。\n",
    "* 定义M的分组：根据不同的特征维度对M进行分组。\n",
    "\n",
    "如果不需要进行分组M_GROUPS列表全设为1即可，如果不需要宏观变量，设置为空列表即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa32ca396e8c328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:21.821065500Z",
     "start_time": "2024-06-02T05:02:21.812879300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 全局变量定义\n",
    "HIDDEN_LAYERS_MODEL_ONE = 2\n",
    "HIDDEN_NODES_MODEL_ONE = [128, 128]\n",
    "OUTPUT_NODES_MODEL_ONE = 32\n",
    "\n",
    "HIDDEN_LAYERS_MODEL_TWO = 3\n",
    "HIDDEN_NODES_MODEL_TWO = [8, 4, 2]\n",
    "OUTPUT_NODES_MODEL_TWO = 1\n",
    "\n",
    "# M 分组的全局变量\n",
    "M_GROUPS = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe392244cbf0a1c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 神经网络结构的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6268d8ca976851ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:23.411937300Z",
     "start_time": "2024-06-02T05:02:23.407931600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model_one(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[64], dropout_rate=0.5, output_nodes=OUTPUT_NODES_MODEL_ONE):\n",
    "        super(Model_one, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        self.norm_layer = nn.BatchNorm1d(hidden_sizes[-1])\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.norm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model_two(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers=HIDDEN_LAYERS_MODEL_TWO, dropout_rate=0.5,\n",
    "                 hidden_nodes=HIDDEN_NODES_MODEL_TWO, output_nodes=OUTPUT_NODES_MODEL_TWO):\n",
    "        super(Model_two, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_nodes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ))\n",
    "\n",
    "        for i in range(1, hidden_layers):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(hidden_nodes[i - 1], hidden_nodes[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ))\n",
    "\n",
    "        self.norm_layer = nn.BatchNorm1d(hidden_nodes[-1])\n",
    "        self.output_layer = nn.Linear(hidden_nodes[-1], output_nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IntegratedModel(nn.Module):\n",
    "    def __init__(self, input_size_model_one, output_size, m_groups, dropout_rate=0.5):\n",
    "        super(IntegratedModel, self).__init__()\n",
    "        self.model_one = Model_one(input_size_model_one, dropout_rate=dropout_rate)\n",
    "\n",
    "        # 创建一个字典，键是组号，值是每个组的Model_two实例\n",
    "        self.model_twos = nn.ModuleDict({\n",
    "            str(group): Model_two(sum(1 for g in m_groups if g == group), dropout_rate=dropout_rate)\n",
    "            for group in set(m_groups)\n",
    "        }) if m_groups else None\n",
    "\n",
    "        # 调整最终输出层的输入尺寸\n",
    "        final_input_size = OUTPUT_NODES_MODEL_ONE + (OUTPUT_NODES_MODEL_TWO * len(self.model_twos) if self.model_twos else 0)\n",
    "        self.final_layer = nn.Linear(final_input_size, output_size)\n",
    "\n",
    "    def forward(self, x1, x2, m_groups):\n",
    "        out_one = self.model_one(x1)\n",
    "\n",
    "        if self.model_twos:\n",
    "            # 获取每个组的Model_two的输出\n",
    "            outs_two = []\n",
    "            for group in set(m_groups):\n",
    "                group_indices = [i for i, g in enumerate(m_groups) if g == group]\n",
    "                group_input = x2[:, group_indices]\n",
    "                group_output = self.model_twos[str(group)](group_input)\n",
    "                outs_two.append(group_output)\n",
    "\n",
    "            combined_out_two = torch.cat(outs_two, dim=1)\n",
    "            combined_out = torch.cat((out_one, combined_out_two), dim=1)\n",
    "        else:\n",
    "            combined_out = out_one\n",
    "\n",
    "        final_out = self.final_layer(combined_out)\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ead06fd52aec2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义超参数搜索逻辑\n",
    "超参数搜索：优化模型的dropout率和L2正则化系数。每次组合不同的dropout率和L2正则化系数（网络搜索），训练模型并评估验证损失，选择最优的超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ceda4c0d085d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:32.051265500Z",
     "start_time": "2024-06-02T05:02:32.046686500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(F_train, M_train, Y_train, dropout_rates, l2_regs, m_groups, batch_size=32):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_val_loss = np.inf\n",
    "    best_params = None\n",
    "    print(\"\\n 参数寻优中，请稍后~\")\n",
    "\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for l2_reg in l2_regs:\n",
    "            model = IntegratedModel(F_train.shape[1], M_train.shape[1] if m_groups else 0, Y_train.shape[1], m_groups, dropout_rate).to(device)\n",
    "            optimizer = optim.AdamW(model.parameters(), weight_decay=l2_reg)\n",
    "            loss_fn = nn.MSELoss()\n",
    "\n",
    "            F_train_sub, F_val, M_train_sub, M_val, Y_train_sub, Y_val = train_test_split(\n",
    "                F_train.cpu().numpy(), M_train.cpu().numpy(), Y_train.cpu().numpy(), test_size=0.15, random_state=3407)\n",
    "\n",
    "            F_train_sub, F_val = torch.tensor(F_train_sub, dtype=torch.float32, device=device), torch.tensor(F_val,\n",
    "                                                                                                             dtype=torch.float32,\n",
    "                                                                                                             device=device)\n",
    "            M_train_sub, M_val = torch.tensor(M_train_sub, dtype=torch.float32, device=device), torch.tensor(M_val,\n",
    "                                                                                                             dtype=torch.float32,\n",
    "                                                                                                             device=device) if m_groups else (None, None)\n",
    "            Y_train_sub, Y_val = torch.tensor(Y_train_sub, dtype=torch.float32, device=device), torch.tensor(Y_val,\n",
    "                                                                                                             dtype=torch.float32,\n",
    "                                                                                                             device=device)\n",
    "\n",
    "            train_dataset = torch.utils.data.TensorDataset(F_train_sub, M_train_sub, Y_train_sub) if m_groups else torch.utils.data.TensorDataset(F_train_sub, Y_train_sub)\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            early_stop_counter = 0\n",
    "            min_val_loss = np.inf\n",
    "\n",
    "            for epoch in tqdm(range(10000), desc=f\"Optimizing: Dropout {dropout_rate}, L2 {l2_reg}\"):\n",
    "                model.train()\n",
    "                for data in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    if m_groups:\n",
    "                        F_batch, M_batch, Y_batch = data\n",
    "                        outputs = model(F_batch, M_batch, m_groups)\n",
    "                    else:\n",
    "                        F_batch, Y_batch = data\n",
    "                        outputs = model(F_batch, None, m_groups)\n",
    "                    loss = loss_fn(outputs, Y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    if m_groups:\n",
    "                        val_outputs = model(F_val, M_val, m_groups)\n",
    "                    else:\n",
    "                        val_outputs = model(F_val, None, m_groups)\n",
    "                    val_loss = loss_fn(val_outputs, Y_val)\n",
    "\n",
    "                if val_loss.item() < min_val_loss:\n",
    "                    min_val_loss = val_loss.item()\n",
    "                    early_stop_counter = 0\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "\n",
    "                if early_stop_counter > 500:\n",
    "                    print(f\"\\n早停，以防过拟合。min_val_loss:{min_val_loss}\")\n",
    "                    break\n",
    "\n",
    "            if min_val_loss < best_val_loss:\n",
    "                best_val_loss = min_val_loss\n",
    "                best_params = {'dropout_rate': dropout_rate, 'l2_reg': l2_reg}\n",
    "\n",
    "    print(f\"\\n最佳超参数：{best_params}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c416694a51000c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义主函数\n",
    "* 读取和处理数据：从CSV文件中读取数据，并进行特征缩放。\n",
    "* 数据分割：将数据分割成训练集和测试集。\n",
    "* 特征缩放：对特征进行归一化处理。\n",
    "* 初始化预测数组：初始化一个全是NaN的数组，用于存储预测结果。\n",
    "* 逐个处理测试样本：每隔48个样本进行一次超参数搜索，并训练多个模型，选择最优模型进行预测。\n",
    "* 保存结果：将预测结果和实际值保存为CSV文件，并计算R2OOS分数。\n",
    "\n",
    "关键参数在于`n_trials=20`, `k_best=2`, 用于控制重复训练几次和取前几个模型的平均值作为预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac179ae08dc71a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:33.391034400Z",
     "start_time": "2024-06-02T05:02:33.374682500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(filename, start_date, end_date, split_date, m_groups, n_trials=20, k_best=2):\n",
    "    base_dir = './jknetwork'\n",
    "    results_dir = './jknetwork/results'\n",
    "    ensure_dir(base_dir)\n",
    "    ensure_dir(results_dir)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Y, X, F, M, time = process_data(filename, start_date, end_date)\n",
    "\n",
    "    # 如果想同时使用X和F特征，取消以下代码的注释\n",
    "    # F = np.hstack((X, F))\n",
    "\n",
    "    # 如果想使用X特征，取消以下代码的注释\n",
    "    # F = X\n",
    "    \n",
    "    Y_tensor = torch.FloatTensor(Y).to(device)\n",
    "    F_tensor = torch.FloatTensor(F).to(device)\n",
    "    M_tensor = torch.FloatTensor(M).to(device)\n",
    "\n",
    "    split_idx = np.where(time == pd.to_datetime(split_date, format='%Y%m'))[0][0]\n",
    "    F_train, F_test = F_tensor[:split_idx], F_tensor[split_idx:]\n",
    "    M_train, M_test = M_tensor[:split_idx], M_tensor[split_idx:]\n",
    "    Y_train, Y_test = Y_tensor[:split_idx], Y_tensor[split_idx:]\n",
    "\n",
    "    scaler_F = MinMaxScaler()\n",
    "    F_train = torch.FloatTensor(scaler_F.fit_transform(F_train.cpu().numpy())).to(device)\n",
    "    F_test = torch.FloatTensor(scaler_F.transform(F_test.cpu().numpy())).to(device)\n",
    "\n",
    "    scaler_M = MinMaxScaler()\n",
    "    M_train = torch.FloatTensor(scaler_M.fit_transform(M_train.cpu().numpy())).to(device) if m_groups else None\n",
    "    M_test = torch.FloatTensor(scaler_M.transform(M_test.cpu().numpy())).to(device) if m_groups else None\n",
    "\n",
    "    Ypre = np.full(Y.shape, np.nan)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in tqdm(range(len(F_test)), desc=\"Processing Test Samples\"):\n",
    "        time_point_dir = os.path.join(base_dir, f'time_point_{i}')\n",
    "        ensure_dir(time_point_dir)\n",
    "        if i % 48 == 0:\n",
    "            dropout_rates = [0.1, 0.3, 0.5]\n",
    "            l2_regs = [0.01, 0.004, 0.007, 0.001]\n",
    "            best_params = hyperparameter_search(F_train, M_train if m_groups else np.zeros((len(F_train), 0)), Y_train, dropout_rates, l2_regs, m_groups, batch_size=32)\n",
    "        trial_models = []\n",
    "        for trial in range(n_trials):\n",
    "            model = IntegratedModel(F_train.shape[1], M_train.shape[1] if m_groups else 0, Y_train.shape[1], m_groups, best_params['dropout_rate']).to(device)\n",
    "            optimizer = optim.AdamW(model.parameters(), weight_decay=best_params['l2_reg'])\n",
    "            loss_fn = nn.MSELoss()\n",
    "\n",
    "            F_train_val, F_val, M_train_val, M_val, Y_train_val, Y_val = train_test_split(\n",
    "                F_train.cpu().numpy(), M_train.cpu().numpy(), Y_train.cpu().numpy(), test_size=0.15, random_state=3407)\n",
    "            F_train_val, F_val = torch.tensor(F_train_val, dtype=torch.float32, device=device), torch.tensor(F_val,\n",
    "                                                                                                             dtype=torch.float32,\n",
    "                                                                                                             device=device)\n",
    "            M_train_val, M_val = torch.tensor(M_train_val, dtype=torch.float32, device=device), torch.tensor(M_val,\n",
    "                                                                                                             dtype=torch.float32,\n",
    "                                                                                                             device=device) if m_groups else (None, None)\n",
    "            Y_train_val, Y_val = torch.tensor(Y_train_val, dtype=torch.float32, device=device), torch.tensor(Y_val,\n",
    "                                                                                                             dtype=torch.float32,\n",
    "                                                                                                             device=device)\n",
    "\n",
    "            train_dataset = torch.utils.data.TensorDataset(F_train_val, M_train_val, Y_train_val) if m_groups else torch.utils.data.TensorDataset(F_train_val, Y_train_val)\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "            min_val_loss = np.inf\n",
    "            early_stop_counter = 0\n",
    "            for epoch in range(10000):\n",
    "                model.train()\n",
    "                for data in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    if m_groups:\n",
    "                        F_batch, M_batch, Y_batch = data\n",
    "                        outputs = model(F_batch, M_batch, m_groups)\n",
    "                    else:\n",
    "                        F_batch, Y_batch = data\n",
    "                        outputs = model(F_batch, None, m_groups)\n",
    "                    loss = loss_fn(outputs, Y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    if m_groups:\n",
    "                        val_outputs = model(F_val, M_val, m_groups)\n",
    "                    else:\n",
    "                        val_outputs = model(F_val, None, m_groups)\n",
    "                    val_loss = loss_fn(val_outputs, Y_val)\n",
    "\n",
    "                if val_loss.item() < min_val_loss:\n",
    "                    min_val_loss = val_loss.item()\n",
    "                    early_stop_counter = 0\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "\n",
    "                if early_stop_counter > 500:\n",
    "                    print(f\"Early stopping at trial {trial + 1}, epoch {epoch + 1} with val loss: {min_val_loss}\")\n",
    "                    break\n",
    "\n",
    "            trial_models.append((min_val_loss, model))\n",
    "\n",
    "        trial_models.sort(key=lambda x: x[0])\n",
    "        best_models = trial_models[:k_best]\n",
    "\n",
    "        model_preds = []\n",
    "        for _, model in best_models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if m_groups:\n",
    "                    test_pred = model(F_test[i:i + 1], M_test[i:i + 1], m_groups)\n",
    "                else:\n",
    "                    test_pred = model(F_test[i:i + 1], None, m_groups)\n",
    "                model_preds.append(test_pred.cpu().numpy())\n",
    "\n",
    "        avg_pred = np.mean(model_preds, axis=0)\n",
    "        predictions.append(avg_pred.flatten().tolist())\n",
    "\n",
    "        Ypre[split_idx + i] = avg_pred\n",
    "\n",
    "        config_path = os.path.join(time_point_dir, 'network_config.txt')\n",
    "        save_model_config(model, config_path)\n",
    "\n",
    "        for rank, (loss, model) in enumerate(best_models):\n",
    "            model_path = os.path.join(time_point_dir, f'best_model_{rank+1}.pt')\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = Y_test.cpu().numpy()\n",
    "    results_df = pd.DataFrame({'Predictions': predictions.flatten(), 'Actuals': actuals.flatten()})\n",
    "    results_df_path = os.path.join(results_dir, 'bigresults3.csv')\n",
    "    results_df.to_csv(results_df_path, index=False)\n",
    "    print(\"\\n预测完成噜，结果已保存至\", results_df_path)\n",
    "\n",
    "    R2OOS_scores_per_dimension = [R2OOS(Y[:, i], Ypre[:, i]) for i in range(predictions.shape[1])]\n",
    "    print(\"R2OOS Scores Per Dimension:\", R2OOS_scores_per_dimension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa5cd6178b1aa6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 一切准备就绪，准备开启炼丹炉！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2efdb68cd5e36da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T05:02:55.628478500Z",
     "start_time": "2024-06-02T05:02:34.158718600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Samples:   0%|          | 0/384 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 参数寻优中，请稍后~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 0/10000 [00:00<?, ?it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 1/10000 [00:00<56:16,  2.96it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 4/10000 [00:00<16:46,  9.94it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 7/10000 [00:00<11:59, 13.88it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 9/10000 [00:00<10:52, 15.32it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 12/10000 [00:00<09:34, 17.37it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 14/10000 [00:00<09:13, 18.04it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 17/10000 [00:01<08:35, 19.35it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 20/10000 [00:01<08:23, 19.81it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 23/10000 [00:01<08:08, 20.42it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 26/10000 [00:01<08:10, 20.34it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 29/10000 [00:01<07:59, 20.79it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 32/10000 [00:01<07:50, 21.16it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 35/10000 [00:01<07:57, 20.88it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 38/10000 [00:02<07:49, 21.20it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 41/10000 [00:02<07:45, 21.41it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 44/10000 [00:02<07:36, 21.80it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 47/10000 [00:02<07:34, 21.88it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   0%|          | 50/10000 [00:02<07:33, 21.94it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 53/10000 [00:02<07:29, 22.15it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 56/10000 [00:02<07:21, 22.52it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 59/10000 [00:03<07:24, 22.38it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 62/10000 [00:03<07:20, 22.57it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 65/10000 [00:03<07:21, 22.50it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 68/10000 [00:03<07:24, 22.33it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 71/10000 [00:03<07:22, 22.45it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 74/10000 [00:03<07:21, 22.48it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 77/10000 [00:03<07:26, 22.22it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 80/10000 [00:03<07:27, 22.18it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 83/10000 [00:04<07:21, 22.45it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 86/10000 [00:04<07:19, 22.54it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 89/10000 [00:04<07:19, 22.53it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 92/10000 [00:04<07:26, 22.18it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 95/10000 [00:04<07:31, 21.95it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 98/10000 [00:04<07:39, 21.56it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 101/10000 [00:04<07:30, 21.99it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 104/10000 [00:05<07:29, 22.00it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 107/10000 [00:05<07:33, 21.83it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 110/10000 [00:05<07:37, 21.61it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 113/10000 [00:05<07:32, 21.83it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 116/10000 [00:05<07:31, 21.90it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 119/10000 [00:05<07:30, 21.93it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|          | 122/10000 [00:05<07:34, 21.74it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 125/10000 [00:06<07:32, 21.84it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 128/10000 [00:06<07:57, 20.68it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 131/10000 [00:06<07:53, 20.84it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 134/10000 [00:06<07:53, 20.84it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 137/10000 [00:06<07:45, 21.20it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 140/10000 [00:06<07:39, 21.45it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 143/10000 [00:06<07:35, 21.62it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 146/10000 [00:07<07:29, 21.94it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   1%|▏         | 149/10000 [00:07<07:20, 22.38it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 152/10000 [00:07<07:19, 22.40it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 155/10000 [00:07<07:26, 22.06it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 158/10000 [00:07<07:26, 22.06it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 161/10000 [00:07<07:33, 21.68it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 164/10000 [00:07<07:32, 21.75it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 167/10000 [00:07<07:26, 22.02it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 170/10000 [00:08<07:26, 22.04it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 173/10000 [00:08<07:31, 21.77it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 176/10000 [00:08<07:26, 21.99it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 179/10000 [00:08<07:26, 22.02it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 182/10000 [00:08<07:29, 21.86it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 185/10000 [00:08<07:27, 21.93it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 188/10000 [00:08<07:20, 22.28it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 191/10000 [00:09<07:18, 22.39it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 194/10000 [00:09<07:15, 22.49it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 197/10000 [00:09<07:14, 22.58it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 200/10000 [00:09<07:23, 22.11it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 203/10000 [00:09<07:20, 22.22it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 206/10000 [00:09<07:25, 21.98it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 209/10000 [00:09<07:21, 22.16it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 212/10000 [00:09<07:25, 21.98it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 215/10000 [00:10<07:29, 21.76it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 218/10000 [00:10<07:31, 21.66it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 221/10000 [00:10<07:25, 21.94it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 224/10000 [00:10<07:28, 21.81it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 227/10000 [00:10<07:30, 21.70it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 230/10000 [00:10<07:28, 21.80it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 233/10000 [00:10<07:27, 21.81it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 236/10000 [00:11<07:23, 22.01it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 239/10000 [00:11<07:26, 21.84it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 242/10000 [00:11<07:30, 21.67it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 245/10000 [00:11<07:27, 21.78it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   2%|▏         | 248/10000 [00:11<07:18, 22.26it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 251/10000 [00:11<07:23, 21.98it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 254/10000 [00:11<07:38, 21.25it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 257/10000 [00:12<07:37, 21.30it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 260/10000 [00:12<07:29, 21.69it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 263/10000 [00:12<07:26, 21.81it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 266/10000 [00:12<07:32, 21.52it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 269/10000 [00:12<07:28, 21.69it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 272/10000 [00:12<07:30, 21.58it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 275/10000 [00:12<07:23, 21.94it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 278/10000 [00:13<07:20, 22.07it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 281/10000 [00:13<07:21, 22.00it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 284/10000 [00:13<07:20, 22.04it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 287/10000 [00:13<07:22, 21.96it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 290/10000 [00:13<07:26, 21.75it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 293/10000 [00:13<07:24, 21.84it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 296/10000 [00:13<07:19, 22.10it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 299/10000 [00:13<07:23, 21.86it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 302/10000 [00:14<07:22, 21.92it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 305/10000 [00:14<07:29, 21.56it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 308/10000 [00:14<07:35, 21.28it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 311/10000 [00:14<07:44, 20.88it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 314/10000 [00:14<07:36, 21.20it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 317/10000 [00:14<07:36, 21.19it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 320/10000 [00:14<07:50, 20.59it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 323/10000 [00:15<07:52, 20.46it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 326/10000 [00:15<07:43, 20.87it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 329/10000 [00:15<07:40, 21.01it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 332/10000 [00:15<07:33, 21.32it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 335/10000 [00:15<07:36, 21.17it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 338/10000 [00:15<07:34, 21.24it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 341/10000 [00:15<07:32, 21.35it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 344/10000 [00:16<07:43, 20.82it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   3%|▎         | 347/10000 [00:16<07:39, 21.00it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 350/10000 [00:16<07:36, 21.15it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 353/10000 [00:16<07:34, 21.23it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 356/10000 [00:16<07:37, 21.10it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 359/10000 [00:16<07:35, 21.19it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 362/10000 [00:16<07:28, 21.50it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 365/10000 [00:17<07:23, 21.75it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 368/10000 [00:17<07:32, 21.29it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 371/10000 [00:17<07:26, 21.58it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▎         | 374/10000 [00:17<07:38, 20.98it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 377/10000 [00:17<07:27, 21.48it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 380/10000 [00:17<07:35, 21.10it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 383/10000 [00:17<07:37, 21.02it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 386/10000 [00:18<07:38, 20.99it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 389/10000 [00:18<07:38, 20.94it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 392/10000 [00:18<07:38, 20.95it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 395/10000 [00:18<07:35, 21.09it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 398/10000 [00:18<07:33, 21.18it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 401/10000 [00:18<07:32, 21.21it/s]\u001B[A\n",
      "Optimizing: Dropout 0.1, L2 0.01:   4%|▍         | 406/10000 [00:19<07:31, 21.24it/s]\u001B[A\n",
      "Processing Test Samples:   0%|          | 0/384 [00:21<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main('./data.csv', '197108', '202112', '199001', M_GROUPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1969d24266daf57",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
